{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch installation\n",
    "```\n",
    "pip install torch torchvision --index-url=download.pytoch.org/whl/cu126\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some imports\n",
    "import torch\n",
    "from ultralytics import YOLO, settings\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import shutil\n",
    "import cv2\n",
    "\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ultralytics settings\n",
    "settings.update({'datasets_dir': './datasets/CCPD'})\n",
    "settings.update({'runs_dir': './runs'})\n",
    "settings.update({'weights_dir': './weights'})\n",
    "\n",
    "# CCPD settings\n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "alphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W','X', 'Y', 'Z', 'O']\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "subsets=['ccpd_base','ccpd_blur','ccpd_challenge','ccpd_db','ccpd_fn','ccpd_rotate','ccpd_tilt','ccpd_weather']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make val datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in Path('datasets/CCPD/ccpd_base').glob('*.txt'):\n",
    "    # delete the label file\n",
    "    label.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subset: ccpd_rotate\n",
      "Finished processing subset: ccpd_rotate, 10053 labels created.\n",
      "Processing subset: ccpd_tilt\n",
      "Finished processing subset: ccpd_tilt, 30216 labels created.\n",
      "Processing subset: ccpd_weather\n",
      "Finished processing subset: ccpd_weather, 9999 labels created.\n"
     ]
    }
   ],
   "source": [
    "# make CCPD labels\n",
    "\n",
    "root = Path('./datasets/CCPD')\n",
    "\n",
    "def xyxy2xywhnormalize(xyxy):\n",
    "    \"\"\"\n",
    "    Convert xyxy to xywh and normalize the coordinates to [0, 1]\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = xyxy\n",
    "    x = (x1 + x2) / (2 * 720)\n",
    "    y = (y1 + y2) / (2 * 1160)\n",
    "    w = (x2 - x1) / 720\n",
    "    h = (y2 - y1) / 1160\n",
    "    return x, y, w, h\n",
    "\n",
    "\n",
    "def getbbox(image_path):\n",
    "    \"\"\"\n",
    "    Get the bounding box coordinates from the image name.\n",
    "    \"\"\"\n",
    "    parts = image_path.stem.split(\"-\")\n",
    "    coords = list(map(int, parts[2].replace(\"&\", \",\").replace(\"_\", \",\").split(\",\")))\n",
    "    return coords\n",
    "\n",
    "\n",
    "for subset in subsets:\n",
    "    counter=0\n",
    "    print(f'Processing subset: {subset}')\n",
    "    subset_path = root / subset\n",
    "    if not subset_path.exists():\n",
    "        continue\n",
    "    labels_path = subset_path/'labels'\n",
    "    if not labels_path.exists():\n",
    "        labels_path.mkdir(parents=True, exist_ok=True)\n",
    "    for image_path in subset_path.glob('*.jpg'):\n",
    "        label_path = labels_path / image_path.name.replace('.jpg', '.txt')\n",
    "        # print(f'0 {\" \".join(map(str, xyxy2xywhnormalize(getbbox(image_path))))}\\n{label_path}')\n",
    "        with open(label_path, 'w') as f:\n",
    "            f.write(f'0 {\" \".join(map(str, xyxy2xywhnormalize(getbbox(image_path))))}\\n')  # dummy label\n",
    "            counter += 1\n",
    "    print(f'Finished processing subset: {subset}, {counter} labels created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copied 20066 images from ccpd_base subset\n",
      "copied 2055 images from ccpd_blur subset\n",
      "copied 5036 images from ccpd_challenge subset\n",
      "copied 1006 images from ccpd_db subset\n",
      "copied 2093 images from ccpd_fn subset\n",
      "copied 989 images from ccpd_rotate subset\n",
      "copied 3013 images from ccpd_tilt subset\n",
      "copied 964 images from ccpd_weather subset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from random import random\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# create test dataset as YOLO dataset format, randomly pick pics from subsets of CCPD into dataset, the root directory name is like dataset-202505212358, the structure of the dataset is like:\n",
    "# dataset-202505212358\n",
    "# ├── images\n",
    "# │   ├── train\n",
    "# │   ├── val\n",
    "# │   └── test\n",
    "# └── labels\n",
    "# │   ├── train\n",
    "# │   ├── val\n",
    "# │   └── test\n",
    "# └── dataset.yaml\n",
    "# becasue of only val this time, so only one folder\n",
    "def create_test_dataset():\n",
    "    datalist={}\n",
    "    dataset_name = 'dataset-' + datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    dataset_path = Path('datasets')/dataset_name\n",
    "    if dataset_path.exists():\n",
    "        shutil.rmtree(dataset_path)\n",
    "    dataset_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # create images and labels directories\n",
    "    images_path = dataset_path / 'images'\n",
    "    labels_path = dataset_path / 'labels'\n",
    "    images_path.mkdir(parents=True, exist_ok=True)\n",
    "    labels_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for subset in subsets:\n",
    "        counter=0\n",
    "        subset_path = Path(f'./datasets/CCPD/{subset}')\n",
    "        subset_labels_path = Path(f'./datasets/CCPD/{subset}/labels')\n",
    "\n",
    "        # copy 10% of the images and labels to the test dataset\n",
    "        for image_file in subset_path.glob('*.jpg'):\n",
    "            if random() < 0.1:\n",
    "                shutil.copy(image_file, images_path/image_file.name)\n",
    "                label_file = subset_labels_path / image_file.name.replace('.jpg', '.txt')\n",
    "                if label_file.exists():\n",
    "                    shutil.copy(label_file, labels_path/label_file.name)\n",
    "                counter+=1\n",
    "        print(f'copied {counter} images from {subset} subset')\n",
    "        datalist[subset] = counter\n",
    "\n",
    "    # create dataset.yaml file\n",
    "    with open(dataset_path / 'dataset.yaml', 'w') as f:\n",
    "        f.write('train: ./images\\n')\n",
    "        f.write('val: ./images\\n')\n",
    "        f.write('test: ./images\\n')\n",
    "        f.write('nc: 1\\n')\n",
    "        f.write('names: [\\'license plate\\']\\n')\n",
    "    \n",
    "    with open(dataset_path/'data.json', 'w') as f:\n",
    "        f.write(json.dumps(datalist))\n",
    "        \n",
    "    print(f'Finished creating test dataset: {dataset_name}, {sum(datalist.values())} images copied.')\n",
    "\n",
    "create_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file = WindowsPath('datasets/CCPD/ccpd_blur/0359-5_21-151&285_417&398-417&398_179&377_151&285_389&306-0_0_4_33_32_25_12-59-4.jpg')\n",
      "parts = ['0359', '5_21', '151&285_417&398', '417&398_179&377_151&285_389&306', '0_0_4_33_32_25_12', '59', '4']\n",
      "cords = [417, 398, 179, 377, 151, 285, 389, 306]\n",
      "points = array([[417, 398],\n",
      "       [179, 377],\n",
      "       [151, 285],\n",
      "       [389, 306]])\n",
      "plate_number_list = ['0', '0', '4', '33', '32', '25', '12']\n",
      "plate_number = '皖A E981N'\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "test_file = './datasets/CCPD/splits/test.txt'\n",
    "datasets_root_dir = Path('./datasets/CCPD')\n",
    "\n",
    "with open(test_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "file = Path(datasets_root_dir / lines[0].strip())\n",
    "print(f'{file = }')\n",
    "\n",
    "parts=Path(datasets_root_dir/lines[0].strip()).stem.split('-')\n",
    "print(f'{parts = }')\n",
    "\n",
    "cords=list(map(int, parts[3].replace(\"&\", \",\").replace(\"_\",\",\").split(\",\")))\n",
    "print(f'{cords = }')\n",
    "\n",
    "points=np.array(cords).reshape(-1, 2)\n",
    "print(f'{points = }')\n",
    "\n",
    "plate_number_list = parts[-3].split('_')\n",
    "print(f'{plate_number_list = }')\n",
    "\n",
    "province_letter = provinces[int(plate_number_list[0])]\n",
    "alphabet_letter = alphabets[int(plate_number_list[1])]\n",
    "number_letter_list = plate_number_list[2:]\n",
    "number_letter = \"\".join([ads[int(char)] for char in number_letter_list])\n",
    "plate_number = province_letter + alphabet_letter + \" \" + number_letter\n",
    "print(f'{plate_number = }')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path, WindowsPath\n",
    "import cv2\n",
    "import numpy as np\n",
    "# CCPD dataloader\n",
    "class CCPDLoader:\n",
    "    def __init__(self, file: str, img_size: int=640):\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        self.img_files = [datasets_root_dir / line.strip() for line in lines]\n",
    "        self.img_size = img_size\n",
    "        # CCPD annotation is it's filename\n",
    "        self.annotations = [self.parse_filename(f) for f in self.img_files]\n",
    "\n",
    "    def parse_filename(self, filename: WindowsPath):\n",
    "        \"\"\"\n",
    "        Parse CCPD filename format:\n",
    "        [Area]-[Tilt Angle]-[Bounding box coordinates]-[Four vertices locations]-[License plate number]-[Brightness]-[Blurriness].jpg\n",
    "        Example: 025-95_113-154&383_386&473-386&473_177&454_154&383_363&402-0_0_22_27_27_33_16-37-15.jpg\n",
    "        \"\"\"\n",
    "        parts = filename.stem.split('-') \n",
    "        coords = list(map(int, parts[3].replace(\"&\", \",\").replace(\"_\", \",\").split(\",\")))\n",
    "        points = np.array(coords, dtype=np.float32).reshape(-1, 2)\n",
    "        plate_number_list = parts[-3].split('_')\n",
    "        province_letter = provinces[int(plate_number_list[0])]\n",
    "        alphabet_letter = alphabets[int(plate_number_list[1])]\n",
    "        number_letter_list = plate_number_list[2:]\n",
    "        number_letter = \"\".join([ads[int(char)] for char in number_letter_list])\n",
    "        plate_number = province_letter + alphabet_letter + \" \" + number_letter\n",
    "        return {\"points\": points, \"plate_number\": plate_number}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = cv2.imread(str(self.img_files[index]))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        # resize with same aspect ratio\n",
    "        r = min(self.img_size / h, self.img_size / w)\n",
    "        new_h, new_w = int(h * r), int(w * r)\n",
    "        img = cv2.resize(img, (new_w, new_h))\n",
    "        img = np.ascontiguousarray(img)\n",
    "        return img, self.annotations[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(lines) = 141982\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "with open('datasets/CCPD/splits/test.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    print(f'{len(lines) = }')\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.139  Python-3.12.6 torch-2.7.0+cu126 CUDA:0 (NVIDIA GeForce RTX 4060 Ti, 8188MiB)\n",
      "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "'/mnt/mydisk/xiaolei/code/plate/plate_detect/ultralytics-main/ultralytics/cfg/datasets/plate.yaml' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m model = YOLO(\u001b[33m'\u001b[39m\u001b[33m./weights/yolov8s.pt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# model.val(data='./datasets/CCPD/splits_test.yaml', split='test')\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\ccpd-new\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:630\u001b[39m, in \u001b[36mModel.val\u001b[39m\u001b[34m(self, validator, **kwargs)\u001b[39m\n\u001b[32m    627\u001b[39m args = {**\u001b[38;5;28mself\u001b[39m.overrides, **custom, **kwargs, \u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[32m    629\u001b[39m validator = (validator \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._smart_load(\u001b[33m\"\u001b[39m\u001b[33mvalidator\u001b[39m\u001b[33m\"\u001b[39m))(args=args, _callbacks=\u001b[38;5;28mself\u001b[39m.callbacks)\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[38;5;28mself\u001b[39m.metrics = validator.metrics\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m validator.metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\ccpd-new\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\ccpd-new\\.venv\\Lib\\site-packages\\ultralytics\\engine\\validator.py:179\u001b[39m, in \u001b[36mBaseValidator.__call__\u001b[39m\u001b[34m(self, trainer, model)\u001b[39m\n\u001b[32m    176\u001b[39m     LOGGER.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSetting batch=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.args.batch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m input of shape (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.args.batch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, 3, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimgsz\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimgsz\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.args.data).rsplit(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[-\u001b[32m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33myaml\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33myml\u001b[39m\u001b[33m\"\u001b[39m}:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[43mcheck_det_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.task == \u001b[33m\"\u001b[39m\u001b[33mclassify\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = check_cls_dataset(\u001b[38;5;28mself\u001b[39m.args.data, split=\u001b[38;5;28mself\u001b[39m.args.split)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\ccpd-new\\.venv\\Lib\\site-packages\\ultralytics\\data\\utils.py:392\u001b[39m, in \u001b[36mcheck_det_dataset\u001b[39m\u001b[34m(dataset, autodownload)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_det_dataset\u001b[39m(dataset, autodownload=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    378\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m \u001b[33;03m    Download, verify, and/or unzip a dataset if not found locally.\u001b[39;00m\n\u001b[32m    380\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m \u001b[33;03m        (dict): Parsed dataset information and paths.\u001b[39;00m\n\u001b[32m    391\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m     file = \u001b[43mcheck_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    394\u001b[39m     \u001b[38;5;66;03m# Download (optional)\u001b[39;00m\n\u001b[32m    395\u001b[39m     extract_dir = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\ccpd-new\\.venv\\Lib\\site-packages\\ultralytics\\utils\\checks.py:554\u001b[39m, in \u001b[36mcheck_file\u001b[39m\u001b[34m(file, suffix, download, download_dir, hard)\u001b[39m\n\u001b[32m    552\u001b[39m files = glob.glob(\u001b[38;5;28mstr\u001b[39m(ROOT / \u001b[33m\"\u001b[39m\u001b[33m**\u001b[39m\u001b[33m\"\u001b[39m / file), recursive=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m glob.glob(\u001b[38;5;28mstr\u001b[39m(ROOT.parent / file))  \u001b[38;5;66;03m# find file\u001b[39;00m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files \u001b[38;5;129;01mand\u001b[39;00m hard:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m does not exist\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(files) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hard:\n\u001b[32m    556\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMultiple files match \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, specify exact path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfiles\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: '/mnt/mydisk/xiaolei/code/plate/plate_detect/ultralytics-main/ultralytics/cfg/datasets/plate.yaml' does not exist"
     ]
    }
   ],
   "source": [
    "# filename=list(Path('./datasets/CCPD/ccpd_base').glob('*.jpg'))[0]\n",
    "# parts=filename.stem.split('-')\n",
    "# parts[-3]\n",
    "# filename=\"025-95_113-154&383_386&473-386&473_177&454_154&383_363&402-0_0_22_27_27_33_16-37-15.jpg\"\n",
    "# filename.split('-')\n",
    "# with open(val_file_list,'r') as f:\n",
    "#     filename=f.readlines()\n",
    "# print(filename[1])\n",
    "# !yolo val model=./weights/yolov8s.pt data=./datasets/CCPD/splits_test.yaml split=\"test\"\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = YOLO('./weights/yolov8s.pt')\n",
    "# model.val(data='./datasets/CCPD/splits_test.yaml', split='test')\n",
    "# model.val()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path=Path().resolve() / 'datasets/CCPD'\n",
    "read_file='datasets/CCPD/splits/train.txt'\n",
    "write_file='datasets/CCPD/test/train.txt'\n",
    "with open(read_file, 'r') as fr:\n",
    "    with open(write_file, 'w') as fw:\n",
    "        lines = fr.readlines()\n",
    "        for line in lines:\n",
    "            line=line.strip()\n",
    "            fw.write(f'{path / line}\\n')\n",
    "\n",
    "\n",
    "\n",
    "# Path().resolve()/'datasets/CCPD'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
